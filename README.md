
Gan : https://arxiv.org/pdf/1406.2661.pdf

Improved Wgan (with gradient penalty) : https://arxiv.org/pdf/1704.00028.pdf

seqGan : Include pretrained and policy gradient method : https://arxiv.org/pdf/1609.05473.pdf

LeakGan : dévoir le h de D au G : include pretraining : https://arxiv.org/pdf/1709.08624.pdf

Differentes methodes d'eval pour gan, peut etre de bonnes infos : https://arxiv.org/pdf/1806.04936.pdf

Adversarial Generation of Natural Language : soft argmax pour l'input du D, implemente un peephole, Bleu 2 Bleu 3 : a bien lire : https://arxiv.org/pdf/1705.10929.pdf


Language Generation with Recurrent Generative Adversarial Networks without Pre-training : Beaucoup de bonne infos a piocher, pas de pretrained, utilise aussi softargmax : https://arxiv.org/pdf/1706.01399.pdf

Intéressant a lire, piocher des idées : unsupervised image captioning : https://arxiv.org/pdf/1811.10787.pdf
EMERGENT TRANSLATIONIN MULTI-AGENT COMMUNICATION : https://arxiv.org/pdf/1710.06922.pdf

purpose of noise : https://www.reddit.com/r/MachineLearning/comments/6k4sqj/discussion_what_is_the_purpose_of_noise_in_a/
use dropout ?

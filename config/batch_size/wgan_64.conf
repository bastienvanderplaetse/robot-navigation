[train]
seed: 0
model_type: WGAN
patience: 400
max_epochs: 100
eval_freq: 0
eval_metrics: bleu,loss
# Tokenization was done with -a parameter of moses tokenizer
eval_beam: 1
eval_batch_size: 32
save_best_metrics: True
eval_max_len: 45
n_checkpoints: 0
l2_reg: 1e-05
gclip=1
optimizer: adam
lr: 0.003
batch_size: 64
save_path: out
tensorboard_dir: ${save_path}/tb_dir
log_score: True
log_score_file: batch_size
criteria: batch_size

[model]
#sat
att_type: mlp
dec_dim: 256
dec_init: zero
emb_dim: 128
dropout: 0.5
bucket_by: en

#nmt
bos_type: emb


direction: feats:Numpy -> label:Numpy, en:Text

[data]
root: data/bison/bison_
train_set: {'en': '${root}captions_train',
            'feats': '${root}regions_train.npy',
            'label': '${root}labels_train.npy'}

val_set: {'en': '${root}captions_val',
            'feats': '${root}regions_val.npy',
            'label': '${root}labels_val.npy'}

test_set: {'en': '${root}captions_test',
            'feats': '${root}regions_test.npy',
            'label': '${root}labels_test.npy'}



[vocabulary]
en: ${data:root}vocab.en

[train]
seed: 0
model_type: GAN
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: bleu,loss
# Tokenization was done with -a parameter of moses tokenizer
eval_beam: 12
eval_batch_size: 32
save_best_metrics: True
eval_max_len: 100
n_checkpoints: 0
l2_reg: 1e-05
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
gclip: 1
optimizer: adam
lr: 0.002
batch_size: 64
save_path: out
tensorboard_dir: ${save_path}/tb_dir

[model]
#sat
att_type: mlp
dec_dim: 128
dec_init: zero
emb_dim: 64
tied_emb: 2way
dropout: 0.0

#nmt
sampler_type: approximate
sched_sampling: 0
bos_type: emb


direction: feats:Numpy -> label:Numpy, en:Text

[data]
root: data
train_set: {'en': '${root}/captions_train',
            'feats': '${root}/regions_train.npy',
            'label': '${root}/labels_train.npy'}

val_set: {'en': '${root}/captions_val',
            'feats': '${root}/regions_val.npy',
            'label': '${root}/labels_val.npy'}

test_set: {'en': '${root}/captions_test',
            'feats': '${root}/regions_test.npy',
            'label': '${root}/labels_test.npy'}



[vocabulary]
en: ${data:root}/vocab.en
[train]
seed: 0
model_type: WGAN
patience: 400
max_epochs: 400
eval_freq: 0
eval_metrics: bleu,loss
# Tokenization was done with -a parameter of moses tokenizer
eval_beam: 1
eval_batch_size: 32
save_best_metrics: True
eval_max_len: 15
n_checkpoints: 0
l2_reg: 1e-05
gclip=1
optimizer: adam
lr: 0.001
batch_size: 64
save_path: out
tensorboard_dir: ${save_path}/tb_dir

[model]
#sat
att_type: mlp
dec_dim: 256
D_init: zero
G_init: random
emb_dim: 128
dropout: 0.1
bucket_by: en

#nmt
bos_type: emb


direction: feats:Numpy -> en:Text

[data]
root: ./data/flickr/
train_set: {'en': '${root}/train.lc.norm.tok.en',
            'feats': '${root}/image_splits/train/train-resnet50-avgpool-r224-c224.npy'}


val_set: {'en': '${root}/val.lc.norm.tok.en',
          'feats': '${root}/image_splits/val/val-resnet50-avgpool-r224-c224.npy'}

###############################################
# Vocabulary files created by nmtpy-build-vocab
# one per each language key
###############################################
[vocabulary]

en: ${data:root}/train.lc.norm.tok.vocab.en
